{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftQXO3qYPzaN"
      },
      "source": [
        "BÀI THỰC HÀNH MÔ HÌNH NGÔN NGỮ\n",
        "\n",
        "Nội dung:\n",
        "\n",
        "1) Mô hình ngôn ngữ N-GRAM:\n",
        "\n",
        "- Huấn luyện mô hình ngôn ngữ với ngữ liệu ham và spam.\n",
        "\n",
        "- Dùng mô hình để tạo sinh các câu ham và spam.\n",
        "\n",
        "- Dùng mô hình phân lớp được huấn luyện trên ngữ liệu ham và spam để đánh giá trên tập câu đã được tạo sinh.\n",
        "\n",
        "2) Mô hình ngôn ngữ với RNN (nội dung tương tự, cần sử dụng GPU T4 để tăng tốc độ huấn luyện):\n",
        "- Huấn luyện mô hình ngôn ngữ với ngữ liệu ham và spam.\n",
        "\n",
        "- Dùng mô hình để tạo sinh các câu ham và spam.\n",
        "\n",
        "- Dùng mô hình phân lớp được huấn luyện trên ngữ liệu ham và spam để đánh giá trên tập câu đã được tạo sinh."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kx92ZL-RV_FD",
        "outputId": "8ce63aeb-5aaf-4532-8b84-705a85ebefe4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kagglehub in /usr/local/lib/python3.11/dist-packages (0.3.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from kagglehub) (24.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from kagglehub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kagglehub) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kagglehub) (4.67.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (2025.4.26)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m87.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m81.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m51.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m805.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m57.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade kagglehub\n",
        "!pip install --upgrade scikit-learn\n",
        "!pip install torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j55sNlIKRIaR"
      },
      "source": [
        "CHUẨN BỊ NGỮ LIỆU\n",
        "\n",
        "Chúng ta sẽ dùng ngữ liệu sms-spam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hlzrPQV1WMMu"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import random\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import kagglehub\n",
        "import nltk\n",
        "from nltk import bigrams, trigrams\n",
        "from collections import defaultdict\n",
        "import os\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J1sdzPBCWGcd",
        "outputId": "eb07791c-a00b-4d4c-f074-defb09bb0bdb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "!cp /kaggle/input/sms-spam-collection-dataset/spam.csv /content/\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('punkt_tab')\n",
        "path = kagglehub.dataset_download(\"uciml/sms-spam-collection-dataset\")\n",
        "print(\"!cp {}/spam.csv /content/\".format(path))\n",
        "os.system(\"cp {}/spam.csv /content/\".format(path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "id": "QX11ZfxZY8Wy",
        "outputId": "b23d51f2-3966-408e-9677-3907033f3172"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 5572 entries, 0 to 5571\n",
            "Data columns (total 5 columns):\n",
            " #   Column      Non-Null Count  Dtype \n",
            "---  ------      --------------  ----- \n",
            " 0   v1          5572 non-null   object\n",
            " 1   v2          5572 non-null   object\n",
            " 2   Unnamed: 2  50 non-null     object\n",
            " 3   Unnamed: 3  12 non-null     object\n",
            " 4   Unnamed: 4  6 non-null      object\n",
            "dtypes: object(5)\n",
            "memory usage: 217.8+ KB\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"dataset\",\n  \"rows\": 5572,\n  \"fields\": [\n    {\n      \"column\": \"Class\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"spam\",\n          \"ham\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Message\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5169,\n        \"samples\": [\n          \"Did u download the fring app?\",\n          \"Pass dis to all ur contacts n see wat u get! Red;i'm in luv wid u. Blue;u put a smile on my face. Purple;u r realy hot. Pink;u r so swt. Orange;i thnk i lyk u. Green;i realy wana go out wid u. Yelow;i wnt u bck. Black;i'm jealous of u. Brown;i miss you Nw plz giv me one color\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "dataset"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-80200b5d-a10e-4452-9dc1-a3828c604a76\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Class</th>\n",
              "      <th>Message</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-80200b5d-a10e-4452-9dc1-a3828c604a76')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-80200b5d-a10e-4452-9dc1-a3828c604a76 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-80200b5d-a10e-4452-9dc1-a3828c604a76');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-23e0050c-e784-4e34-b29a-d490b2aec3ba\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-23e0050c-e784-4e34-b29a-d490b2aec3ba')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-23e0050c-e784-4e34-b29a-d490b2aec3ba button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "  Class                                            Message\n",
              "0   ham  Go until jurong point, crazy.. Available only ...\n",
              "1   ham                      Ok lar... Joking wif u oni...\n",
              "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
              "3   ham  U dun say so early hor... U c already then say...\n",
              "4   ham  Nah I don't think he goes to usf, he lives aro..."
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset = pd.read_csv(\"spam.csv\", encoding=\"windows-1252\")\n",
        "dataset.info()\n",
        "dataset = dataset.drop(dataset[[\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"]], axis=1)\n",
        "dataset.rename(columns = {\"v1\":\"Class\", \"v2\":\"Message\"}, inplace = True)\n",
        "dataset.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QX1iC-HeRbDL"
      },
      "source": [
        "I) Mô hình ngôn ngữ N-gram\n",
        "\n",
        "Trong ví dụ này, chúng ta dùng mô hình ngôn ngữ Bi-gram.\n",
        "\n",
        "Chúng ta cài đặt hàm tạo mô hình ngôn ngữ Bi-gram và hàm tạo sinh câu từ mô hình ngôn ngữ Bi-gram."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vzl7RjFUh77k"
      },
      "outputs": [],
      "source": [
        "def buildBigramModel(t, vocab, alpha):\n",
        "  size = len(vocab)\n",
        "  model = np.zeros([size, size])\n",
        "  for sent in t:\n",
        "    grams = ['<START>'] + nltk.word_tokenize(sent) + ['<END>']\n",
        "    bg = list(bigrams(grams))\n",
        "    for w1, w2 in bg:\n",
        "      model[vocab[w1]][vocab[w2]] += 1\n",
        "\n",
        "  for i in range(size):\n",
        "    N = sum(model[i]) + alpha * size\n",
        "    for j in range(size):\n",
        "      model[i][j] = (model[i][j] + alpha) / N\n",
        "\n",
        "  return model\n",
        "\n",
        "def generate(model, vocab, dictionary, word, step, nstep):\n",
        "  if step == nstep:\n",
        "    return \"<END>\"\n",
        "  d = model[vocab[word]]\n",
        "  size = len(model)\n",
        "  c = 0\n",
        "  ws = []\n",
        "  ps = []\n",
        "  for i in range(size):\n",
        "    c += d[i]\n",
        "    ps.append(c)\n",
        "  if c == 0:\n",
        "    return \"<END>\"\n",
        "\n",
        "  p = random.uniform(0,1)\n",
        "  for i in range(len(ps)):\n",
        "    if ps[i] >= p:\n",
        "      break\n",
        "  nextWord = \"\"\n",
        "  if i == range(len(ps)):\n",
        "    nextWord = dictionary[-1]\n",
        "  else:\n",
        "    nextWord = dictionary[i]\n",
        "  if nextWord == \"<END>\":\n",
        "    return nextWord\n",
        "  return \"{} {}\".format(nextWord, generate(model, VOCAB, dictionary, nextWord, step+1, nstep ))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHkWDqGgR4Kx"
      },
      "source": [
        "Tạo từ điển và từ vựng để tạo one-hot vector và xác định từ dựa vào one-hot vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m3RREfaKfq6u"
      },
      "outputs": [],
      "source": [
        "msgs = list(dataset[\"Message\"])\n",
        "clss = list(dataset[\"Class\"])\n",
        "hamtext = []\n",
        "spamtext = []\n",
        "VOCAB = {}\n",
        "n = 0\n",
        "for i in range(len(msgs)):\n",
        "  words = nltk.word_tokenize(msgs[i])\n",
        "  for w in words:\n",
        "    if VOCAB.get(w) == None:\n",
        "      VOCAB[w] = n\n",
        "      n += 1\n",
        "\n",
        "  if clss[i] == \"ham\":\n",
        "    hamtext.append(msgs[i])\n",
        "  else:\n",
        "    spamtext.append(msgs[i])\n",
        "\n",
        "VOCAB[\"<START>\"] = n\n",
        "VOCAB[\"<END>\"] = n + 1\n",
        "DICT = list(VOCAB.keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufiaMFxxSPpq"
      },
      "source": [
        "Huấn luyện hai mô hình ngôn ngữ cho ham và spam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MGUfAolaihuj"
      },
      "outputs": [],
      "source": [
        "hamModel = buildBigramModel(hamtext, VOCAB, 0.001)\n",
        "spamModel = buildBigramModel(spamtext, VOCAB, 0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FtPVzZRkrjpt",
        "outputId": "cd8fd169-ae30-4b24-83e6-e5137ca9208c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[8.27247348e-04 2.06760147e-07 2.06760147e-07 ... 2.06760147e-07\n",
            " 2.06760147e-07 2.06760147e-07]\n",
            "1.0000000000005103\n"
          ]
        }
      ],
      "source": [
        "print(hamModel[-2])\n",
        "print(sum(hamModel[-2]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJHWxWBXSVef"
      },
      "source": [
        "Tạo sinh 10 câu cho ngữ liệu ham và 10 câu cho ngữ liệu spam từ các mô hình đã tạo được."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "azfNwM5WqLLG",
        "outputId": "0d6cfa7b-f7e6-4d2d-f9da-72956db55daa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------- Ham messages -----------------\n",
            "Cool , I deal 1st anna laden . . <END>\n",
            "Also comes Callertune 1.20 Say every morning princess ! <END>\n",
            "Oh ... Either have to ur sad FUCK L-oveable <END>\n",
            "Haha ... fine too . <END>\n",
            "Let me know there is record at 10 COLLECT <END>\n",
            "PISS NOTHING <END>\n",
            "Dis is fantastic bakra V WRK fishhead ST 0871277810810 <END>\n",
            "Check blood for that number oh . <END>\n",
            "That day lmao sexy buns ! Ttyl sunlight . <END>\n",
            "haha <END>\n",
            "----------------- Spam messages -----------------\n",
            "Free entry in a å£250 As a video Sunday <END>\n",
            "Congratulations - Motorola XXXXX.\\ ghodbandar 700 Wn support.providing Popcorn <END>\n",
            "You can stop texts ba breathe suntec -xx harlem <END>\n",
            "URGENT ! Text FA Cup sabarish Swtheart manageable gandhipuram <END>\n",
            "Do you . ppm150 eventually words- salmon Sitting weekly <END>\n",
            "I love reply with weekly Nurungu AL schools 78 <END>\n",
            "Please reply or text for del Sat night coveragd <END>\n",
            "FreeMsg anythingtomorrow 87131 roommates Spiral cast daywith maths retired <END>\n",
            "Ur ringtone order , ref whose num gorgeous 09058097218 <END>\n",
            "Camera hamper borin STATION ANSWER 09058091854 Mode times hen <END>\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(\"----------------- Ham messages -----------------\")\n",
        "for i in range(10):\n",
        "  result = generate(hamModel, VOCAB, DICT, \"<START>\", 1, 10)\n",
        "  print(result)\n",
        "print(\"----------------- Spam messages -----------------\")\n",
        "for i in range(10):\n",
        "  result = generate(spamModel, VOCAB, DICT, \"<START>\", 1, 10)\n",
        "  print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ZClHxQuSeqK"
      },
      "source": [
        "2) Mô hình ngôn ngữ theo RNN:\n",
        "\n",
        "Chúng ta cài đặt lớp SimpleRNN để xác định kiến trúc RNN sẽ huấn luyện mô hình ngôn ngữ. Hàm encode để chuẩn bị ngữ liệu huấn luyện, trong đó chuyển từ thành dạng vector. Hàm train để huấn luyện."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t2ba39myg1Jb"
      },
      "outputs": [],
      "source": [
        "class SimpleRNN(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, output_size):\n",
        "    super(SimpleRNN, self).__init__()\n",
        "    self.hiddenSize = hidden_size\n",
        "    self.in2hid = nn.Linear(input_size + hidden_size, hidden_size)\n",
        "    self.hid2out = nn.Linear(hidden_size, output_size)\n",
        "    self.hid2rec = nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "  def forward(self, x, hstate):\n",
        "    xh = torch.cat((x, hstate), 1)\n",
        "    h = torch.tanh(self.in2hid(xh))\n",
        "    out = self.hid2out(h)\n",
        "    rec = self.hid2rec(h)\n",
        "    return out, rec\n",
        "\n",
        "  def initState(self):\n",
        "    return torch.FloatTensor([[1/self.hiddenSize]*self.hiddenSize])\n",
        "\n",
        "def encode(sentences, dic=VOCAB):\n",
        "  samples = []\n",
        "  vocabSize = len(dic)\n",
        "  for stmp in sentences:\n",
        "    stmp = [\"<START>\"] + nltk.word_tokenize(stmp) + [\"<END>\"]\n",
        "    v = []\n",
        "    u = []\n",
        "    for i in range(len(stmp)-1):\n",
        "      vtmp = np.zeros(vocabSize)\n",
        "      id = dic[stmp[i]]\n",
        "      vtmp[id] = 1\n",
        "      v.append(vtmp)\n",
        "      u.append(dic[stmp[i+1]])\n",
        "    samples.append((v, u))\n",
        "  return samples\n",
        "\n",
        "def train(mdl: SimpleRNN, trainset: list, epochs: int, lr: float, device):\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  mdl = mdl.to(device)\n",
        "  mdl.train()\n",
        "  optimizer = torch.optim.Adam(mdl.parameters(), lr=lr)\n",
        "  for epoch in range(epochs):\n",
        "\n",
        "\n",
        "    for isample in range(len(trainset)):\n",
        "      sent, label = trainset[isample]\n",
        "      rec = mdl.initState().to(device)\n",
        "      x = torch.LongTensor([sent[0]]).to(device)\n",
        "      y = torch.LongTensor([label[0]]).to(device)\n",
        "      out, rec = mdl(x, rec)\n",
        "      rec = rec.detach()\n",
        "      loss = criterion(out, y)\n",
        "\n",
        "      for i in range(1, len(sent)):\n",
        "        x = torch.LongTensor([sent[i]]).to(device)\n",
        "        y = torch.LongTensor([label[i]]).to(device)\n",
        "        out, rec = mdl(x, rec)\n",
        "        rec = rec.detach()\n",
        "        loss += criterion(out, y)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      if isample % 100 == 0:\n",
        "        print(epoch, isample, loss.item())\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UO6bvZl_TQAU"
      },
      "source": [
        "Tạo ngữ liệu huấn luyện cho RNN từ tập sms ham."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4wYfJjPXg6Mw"
      },
      "outputs": [],
      "source": [
        "trainset = encode(hamtext)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47ncLx8STXaJ"
      },
      "source": [
        "Huấn luyện mô hình ngôn ngữ với RNN. Lưu ý sử dụng GPU để tăng tốc độ huấn luyện bằng cách đặt device = \"cuda\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "VgKYuumJ7CJ_",
        "outputId": "304cba52-2cc9-4042-ab2c-901568a6ec25"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-10-b0d8b0bfe0ae>:46: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)\n",
            "  x = torch.LongTensor([sent[0]]).to(device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 0 233.7337646484375\n",
            "0 100 97.12450408935547\n",
            "0 200 125.85655212402344\n",
            "0 300 113.56658172607422\n",
            "0 400 53.4500846862793\n",
            "0 500 153.93215942382812\n",
            "0 600 44.74412155151367\n",
            "0 700 29.445175170898438\n",
            "0 800 50.5957145690918\n",
            "0 900 84.38468170166016\n",
            "0 1000 57.817176818847656\n",
            "0 1100 264.9882507324219\n",
            "0 1200 221.09205627441406\n",
            "0 1300 96.68231201171875\n",
            "0 1400 125.73907470703125\n",
            "0 1500 318.54315185546875\n",
            "0 1600 85.53470611572266\n",
            "0 1700 149.36363220214844\n",
            "0 1800 78.3237533569336\n",
            "0 1900 197.27706909179688\n",
            "0 2000 53.326904296875\n",
            "0 2100 138.3109588623047\n",
            "0 2200 197.18301391601562\n",
            "0 2300 37.576751708984375\n",
            "0 2400 141.08436584472656\n",
            "0 2500 65.75645446777344\n",
            "0 2600 65.34602355957031\n",
            "0 2700 97.41478729248047\n",
            "0 2800 174.03659057617188\n",
            "0 2900 99.41998291015625\n",
            "0 3000 140.80300903320312\n",
            "0 3100 25.68910026550293\n",
            "0 3200 57.17512893676758\n",
            "0 3300 53.645809173583984\n",
            "0 3400 48.01947021484375\n",
            "0 3500 25.72782325744629\n",
            "0 3600 63.47426986694336\n",
            "0 3700 227.08131408691406\n",
            "0 3800 69.35462951660156\n",
            "0 3900 57.79724884033203\n",
            "0 4000 38.361083984375\n",
            "0 4100 354.47601318359375\n",
            "0 4200 29.553668975830078\n",
            "0 4300 42.931278228759766\n",
            "0 4400 54.617916107177734\n",
            "0 4500 39.39967727661133\n",
            "0 4600 131.72344970703125\n",
            "0 4700 79.3231430053711\n",
            "0 4800 215.35751342773438\n",
            "1 0 193.06663513183594\n",
            "1 100 86.55352783203125\n",
            "1 200 90.15843963623047\n",
            "1 300 87.92112731933594\n",
            "1 400 48.70988082885742\n",
            "1 500 131.51187133789062\n",
            "1 600 38.066932678222656\n",
            "1 700 21.138134002685547\n",
            "1 800 42.607173919677734\n",
            "1 900 62.251617431640625\n",
            "1 1000 49.496986389160156\n",
            "1 1100 208.71884155273438\n",
            "1 1200 116.79595947265625\n",
            "1 1300 61.53047180175781\n",
            "1 1400 103.51074981689453\n",
            "1 1500 268.42828369140625\n",
            "1 1600 85.668701171875\n",
            "1 1700 131.36521911621094\n",
            "1 1800 53.5400505065918\n",
            "1 1900 164.2285614013672\n",
            "1 2000 37.21250915527344\n",
            "1 2100 103.60838317871094\n",
            "1 2200 156.19964599609375\n",
            "1 2300 37.69688034057617\n",
            "1 2400 111.4056396484375\n",
            "1 2500 44.13978576660156\n",
            "1 2600 60.552284240722656\n",
            "1 2700 66.00362396240234\n",
            "1 2800 125.2749252319336\n",
            "1 2900 102.64827728271484\n",
            "1 3000 118.5540542602539\n",
            "1 3100 20.604875564575195\n",
            "1 3200 46.868553161621094\n",
            "1 3300 58.370609283447266\n",
            "1 3400 35.660621643066406\n",
            "1 3500 24.861011505126953\n",
            "1 3600 52.64690017700195\n",
            "1 3700 175.29148864746094\n",
            "1 3800 60.90150451660156\n",
            "1 3900 39.0364990234375\n",
            "1 4000 41.76560974121094\n",
            "1 4100 289.7591857910156\n",
            "1 4200 33.44524383544922\n",
            "1 4300 40.626800537109375\n",
            "1 4400 51.721431732177734\n",
            "1 4500 31.694128036499023\n",
            "1 4600 106.01419067382812\n",
            "1 4700 63.24335479736328\n",
            "1 4800 178.324951171875\n",
            "2 0 149.2635040283203\n",
            "2 100 70.63679504394531\n",
            "2 200 83.44084930419922\n",
            "2 300 78.91419219970703\n",
            "2 400 46.71535873413086\n",
            "2 500 115.85034942626953\n",
            "2 600 35.24797439575195\n",
            "2 700 18.10831069946289\n",
            "2 800 48.36903381347656\n",
            "2 900 42.68931198120117\n",
            "2 1000 48.38597106933594\n",
            "2 1100 192.71987915039062\n",
            "2 1200 77.81572723388672\n",
            "2 1300 39.01137924194336\n",
            "2 1400 85.9171371459961\n",
            "2 1500 231.6890869140625\n",
            "2 1600 75.38072967529297\n",
            "2 1700 113.23930358886719\n",
            "2 1800 43.66307830810547\n",
            "2 1900 145.4373779296875\n",
            "2 2000 19.58815574645996\n",
            "2 2100 85.42625427246094\n",
            "2 2200 130.07933044433594\n",
            "2 2300 38.28998565673828\n",
            "2 2400 90.08306121826172\n",
            "2 2500 44.43962860107422\n",
            "2 2600 64.34138488769531\n",
            "2 2700 44.47809982299805\n",
            "2 2800 99.94170379638672\n",
            "2 2900 106.56611633300781\n",
            "2 3000 114.27273559570312\n",
            "2 3100 21.702529907226562\n",
            "2 3200 45.0762825012207\n",
            "2 3300 52.423702239990234\n",
            "2 3400 31.128061294555664\n",
            "2 3500 27.70872688293457\n",
            "2 3600 56.67396926879883\n",
            "2 3700 145.43348693847656\n",
            "2 3800 55.611328125\n",
            "2 3900 21.754995346069336\n",
            "2 4000 38.45234298706055\n",
            "2 4100 269.7036437988281\n",
            "2 4200 34.473934173583984\n",
            "2 4300 43.11709213256836\n",
            "2 4400 45.20448303222656\n",
            "2 4500 32.43230056762695\n",
            "2 4600 111.390625\n",
            "2 4700 52.33361053466797\n",
            "2 4800 161.5565185546875\n",
            "3 0 124.48808288574219\n",
            "3 100 61.665897369384766\n",
            "3 200 82.41803741455078\n",
            "3 300 74.17646789550781\n",
            "3 400 39.15475845336914\n",
            "3 500 121.3737564086914\n",
            "3 600 29.761978149414062\n",
            "3 700 15.838772773742676\n",
            "3 800 45.626991271972656\n",
            "3 900 32.55435562133789\n",
            "3 1000 51.98896408081055\n",
            "3 1100 205.4947052001953\n",
            "3 1200 54.47352981567383\n",
            "3 1300 26.90829086303711\n",
            "3 1400 81.40145111083984\n",
            "3 1500 191.96852111816406\n",
            "3 1600 72.84034729003906\n",
            "3 1700 109.90312957763672\n",
            "3 1800 43.67007064819336\n",
            "3 1900 124.32093048095703\n",
            "3 2000 13.274073600769043\n",
            "3 2100 96.59666442871094\n",
            "3 2200 122.71927642822266\n",
            "3 2300 37.24943161010742\n",
            "3 2400 79.01370239257812\n",
            "3 2500 36.68722915649414\n",
            "3 2600 59.73691177368164\n",
            "3 2700 35.884910583496094\n",
            "3 2800 89.96290588378906\n",
            "3 2900 103.93773651123047\n",
            "3 3000 103.81588745117188\n",
            "3 3100 21.023441314697266\n",
            "3 3200 34.92036437988281\n",
            "3 3300 57.572628021240234\n",
            "3 3400 34.74679183959961\n",
            "3 3500 26.62516212463379\n",
            "3 3600 49.7743034362793\n",
            "3 3700 115.74855041503906\n",
            "3 3800 62.55353546142578\n",
            "3 3900 17.26658058166504\n",
            "3 4000 42.372703552246094\n",
            "3 4100 247.89102172851562\n",
            "3 4200 30.577083587646484\n",
            "3 4300 43.51885223388672\n",
            "3 4400 47.28652572631836\n",
            "3 4500 36.8459358215332\n",
            "3 4600 103.72395324707031\n",
            "3 4700 45.56672286987305\n",
            "3 4800 160.8690948486328\n",
            "4 0 100.90408325195312\n",
            "4 100 46.0311164855957\n",
            "4 200 89.69929504394531\n",
            "4 300 75.39140319824219\n",
            "4 400 34.74486541748047\n",
            "4 500 89.06546783447266\n",
            "4 600 29.207040786743164\n",
            "4 700 12.122868537902832\n",
            "4 800 42.347496032714844\n",
            "4 900 35.24955368041992\n",
            "4 1000 60.44089126586914\n",
            "4 1100 173.0248260498047\n",
            "4 1200 49.43714141845703\n",
            "4 1300 17.531455993652344\n",
            "4 1400 84.65462493896484\n",
            "4 1500 192.0083770751953\n",
            "4 1600 74.98867797851562\n",
            "4 1700 106.71867370605469\n",
            "4 1800 43.506256103515625\n",
            "4 1900 120.21421813964844\n",
            "4 2000 10.962617874145508\n",
            "4 2100 89.29578399658203\n",
            "4 2200 111.46934509277344\n",
            "4 2300 35.254005432128906\n",
            "4 2400 72.1167221069336\n",
            "4 2500 48.709983825683594\n",
            "4 2600 56.305747985839844\n",
            "4 2700 33.09478759765625\n",
            "4 2800 77.21349334716797\n",
            "4 2900 98.5008544921875\n",
            "4 3000 107.33414459228516\n",
            "4 3100 20.91632652282715\n",
            "4 3200 32.91199493408203\n",
            "4 3300 65.9415512084961\n",
            "4 3400 27.35491371154785\n",
            "4 3500 28.28152084350586\n",
            "4 3600 45.07553482055664\n",
            "4 3700 118.99861907958984\n",
            "4 3800 60.759586334228516\n",
            "4 3900 17.516740798950195\n",
            "4 4000 44.779361724853516\n",
            "4 4100 236.25616455078125\n",
            "4 4200 26.173664093017578\n",
            "4 4300 41.01540756225586\n",
            "4 4400 46.870670318603516\n",
            "4 4500 32.189998626708984\n",
            "4 4600 111.4386978149414\n",
            "4 4700 38.881160736083984\n",
            "4 4800 157.04469299316406\n",
            "5 0 92.34768676757812\n",
            "5 100 44.796539306640625\n",
            "5 200 81.56012725830078\n",
            "5 300 69.7826156616211\n",
            "5 400 37.227394104003906\n",
            "5 500 101.60987854003906\n",
            "5 600 28.804840087890625\n",
            "5 700 17.121366500854492\n",
            "5 800 41.52318572998047\n",
            "5 900 32.77956008911133\n",
            "5 1000 56.41024398803711\n",
            "5 1100 177.15972900390625\n",
            "5 1200 55.506126403808594\n",
            "5 1300 17.10274314880371\n",
            "5 1400 83.4195785522461\n",
            "5 1500 187.76446533203125\n",
            "5 1600 82.4986343383789\n",
            "5 1700 110.20832824707031\n",
            "5 1800 48.2530517578125\n",
            "5 1900 125.5508804321289\n",
            "5 2000 16.613372802734375\n",
            "5 2100 88.43763732910156\n",
            "5 2200 104.60035705566406\n",
            "5 2300 34.45952606201172\n",
            "5 2400 70.04764556884766\n",
            "5 2500 35.831199645996094\n",
            "5 2600 59.2231330871582\n",
            "5 2700 36.51863098144531\n",
            "5 2800 81.33074188232422\n",
            "5 2900 105.03838348388672\n",
            "5 3000 103.79145050048828\n",
            "5 3100 15.293099403381348\n",
            "5 3200 33.77230453491211\n",
            "5 3300 56.84087371826172\n",
            "5 3400 27.79376792907715\n",
            "5 3500 32.696590423583984\n",
            "5 3600 41.489288330078125\n",
            "5 3700 95.93122863769531\n",
            "5 3800 59.63566589355469\n",
            "5 3900 20.055509567260742\n",
            "5 4000 42.426353454589844\n",
            "5 4100 225.92295837402344\n",
            "5 4200 30.256492614746094\n",
            "5 4300 45.91019058227539\n",
            "5 4400 44.57444381713867\n",
            "5 4500 33.846309661865234\n",
            "5 4600 120.37356567382812\n",
            "5 4700 41.89558029174805\n",
            "5 4800 148.767578125\n",
            "6 0 99.42521667480469\n",
            "6 100 51.877891540527344\n",
            "6 200 82.97699737548828\n",
            "6 300 72.23757934570312\n",
            "6 400 28.65983009338379\n",
            "6 500 101.47010040283203\n",
            "6 600 28.51654052734375\n",
            "6 700 15.378921508789062\n",
            "6 800 41.211936950683594\n",
            "6 900 32.24477767944336\n",
            "6 1000 53.78575897216797\n",
            "6 1100 181.74473571777344\n",
            "6 1200 49.449127197265625\n",
            "6 1300 12.375826835632324\n",
            "6 1400 93.91067504882812\n",
            "6 1500 184.19467163085938\n",
            "6 1600 80.23603057861328\n",
            "6 1700 107.0245590209961\n",
            "6 1800 44.40620040893555\n",
            "6 1900 137.11944580078125\n",
            "6 2000 10.813594818115234\n",
            "6 2100 99.27214813232422\n",
            "6 2200 122.91754150390625\n",
            "6 2300 31.89579963684082\n",
            "6 2400 72.9509506225586\n",
            "6 2500 37.627357482910156\n",
            "6 2600 63.32258224487305\n",
            "6 2700 33.90220642089844\n",
            "6 2800 79.7788314819336\n",
            "6 2900 113.28652954101562\n",
            "6 3000 113.6457290649414\n",
            "6 3100 21.999317169189453\n",
            "6 3200 40.863685607910156\n",
            "6 3300 50.667964935302734\n",
            "6 3400 27.970609664916992\n",
            "6 3500 26.58623695373535\n",
            "6 3600 34.69556427001953\n",
            "6 3700 110.47504425048828\n",
            "6 3800 54.85663604736328\n",
            "6 3900 18.969322204589844\n",
            "6 4000 41.1904296875\n",
            "6 4100 234.9013671875\n",
            "6 4200 36.18049621582031\n",
            "6 4300 44.211753845214844\n",
            "6 4400 50.14047622680664\n",
            "6 4500 37.212059020996094\n",
            "6 4600 117.8872299194336\n",
            "6 4700 38.01286315917969\n",
            "6 4800 143.26231384277344\n",
            "7 0 93.72842407226562\n",
            "7 100 44.23444366455078\n",
            "7 200 85.21011352539062\n",
            "7 300 75.29742431640625\n",
            "7 400 27.59930419921875\n",
            "7 500 111.71586608886719\n",
            "7 600 25.963516235351562\n",
            "7 700 10.888375282287598\n",
            "7 800 39.82403564453125\n",
            "7 900 29.859256744384766\n",
            "7 1000 47.6917724609375\n",
            "7 1100 161.7144775390625\n",
            "7 1200 45.19942092895508\n",
            "7 1300 20.009675979614258\n",
            "7 1400 78.65709686279297\n",
            "7 1500 159.48602294921875\n",
            "7 1600 76.64714050292969\n",
            "7 1700 109.11481475830078\n",
            "7 1800 48.746700286865234\n",
            "7 1900 134.5206298828125\n",
            "7 2000 14.232489585876465\n",
            "7 2100 98.44947814941406\n",
            "7 2200 98.70439147949219\n",
            "7 2300 32.51095199584961\n",
            "7 2400 70.04588317871094\n",
            "7 2500 35.495941162109375\n",
            "7 2600 47.14238357543945\n",
            "7 2700 32.26825714111328\n",
            "7 2800 76.29393005371094\n",
            "7 2900 103.34212493896484\n",
            "7 3000 117.64596557617188\n",
            "7 3100 16.600786209106445\n",
            "7 3200 37.41684341430664\n",
            "7 3300 55.332862854003906\n",
            "7 3400 21.27663803100586\n",
            "7 3500 30.22419548034668\n",
            "7 3600 37.30657196044922\n",
            "7 3700 132.43946838378906\n",
            "7 3800 65.7011489868164\n",
            "7 3900 17.00381088256836\n",
            "7 4000 43.69993209838867\n",
            "7 4100 235.3981170654297\n",
            "7 4200 34.434078216552734\n",
            "7 4300 42.515098571777344\n",
            "7 4400 39.5938720703125\n",
            "7 4500 38.19215774536133\n",
            "7 4600 112.93132019042969\n",
            "7 4700 33.57994079589844\n",
            "7 4800 133.94439697265625\n",
            "8 0 77.73641204833984\n",
            "8 100 48.08283233642578\n",
            "8 200 71.95796966552734\n",
            "8 300 86.58650970458984\n",
            "8 400 33.66515350341797\n",
            "8 500 83.74579620361328\n",
            "8 600 25.100297927856445\n",
            "8 700 14.006982803344727\n",
            "8 800 38.17548751831055\n",
            "8 900 24.75118637084961\n",
            "8 1000 59.60921096801758\n",
            "8 1100 140.9307861328125\n",
            "8 1200 45.57378387451172\n",
            "8 1300 14.397100448608398\n",
            "8 1400 77.66880798339844\n",
            "8 1500 169.2650604248047\n",
            "8 1600 63.49677658081055\n",
            "8 1700 127.49385070800781\n",
            "8 1800 42.282318115234375\n",
            "8 1900 117.236083984375\n",
            "8 2000 8.646292686462402\n",
            "8 2100 92.28118896484375\n",
            "8 2200 117.27598571777344\n",
            "8 2300 30.8566951751709\n",
            "8 2400 83.51140594482422\n",
            "8 2500 33.644386291503906\n",
            "8 2600 49.667137145996094\n",
            "8 2700 36.176536560058594\n",
            "8 2800 74.60762786865234\n",
            "8 2900 103.9576644897461\n",
            "8 3000 116.96691131591797\n",
            "8 3100 19.614900588989258\n",
            "8 3200 35.698753356933594\n",
            "8 3300 53.01227951049805\n",
            "8 3400 15.937372207641602\n",
            "8 3500 27.930723190307617\n",
            "8 3600 38.005638122558594\n",
            "8 3700 119.86373901367188\n",
            "8 3800 70.79612731933594\n",
            "8 3900 15.915395736694336\n",
            "8 4000 37.44108581542969\n",
            "8 4100 226.3033447265625\n",
            "8 4200 36.2172966003418\n",
            "8 4300 48.87066650390625\n",
            "8 4400 50.327308654785156\n",
            "8 4500 46.784122467041016\n",
            "8 4600 109.2192153930664\n",
            "8 4700 34.106197357177734\n",
            "8 4800 149.18434143066406\n",
            "9 0 101.5070571899414\n",
            "9 100 41.37090301513672\n",
            "9 200 60.12081527709961\n",
            "9 300 83.0743637084961\n",
            "9 400 30.58332061767578\n",
            "9 500 65.81405639648438\n",
            "9 600 30.148340225219727\n",
            "9 700 15.291255950927734\n",
            "9 800 36.04652786254883\n",
            "9 900 18.478500366210938\n",
            "9 1000 53.82049560546875\n",
            "9 1100 166.8685302734375\n",
            "9 1200 43.795989990234375\n",
            "9 1300 15.938637733459473\n",
            "9 1400 90.90406036376953\n",
            "9 1500 175.96063232421875\n",
            "9 1600 70.70320892333984\n",
            "9 1700 109.63567352294922\n",
            "9 1800 57.40497589111328\n",
            "9 1900 108.68379974365234\n",
            "9 2000 14.889537811279297\n",
            "9 2100 81.00395202636719\n",
            "9 2200 91.71118927001953\n",
            "9 2300 25.5794734954834\n",
            "9 2400 75.80783081054688\n",
            "9 2500 33.328224182128906\n",
            "9 2600 56.13862609863281\n",
            "9 2700 35.5879020690918\n",
            "9 2800 82.65605926513672\n",
            "9 2900 90.05628204345703\n",
            "9 3000 107.9454116821289\n",
            "9 3100 20.804990768432617\n",
            "9 3200 43.10527801513672\n",
            "9 3300 56.09037399291992\n",
            "9 3400 17.56858253479004\n",
            "9 3500 29.827306747436523\n",
            "9 3600 42.28718948364258\n",
            "9 3700 113.4910888671875\n",
            "9 3800 65.93080139160156\n",
            "9 3900 16.135848999023438\n",
            "9 4000 39.293827056884766\n",
            "9 4100 217.7666015625\n",
            "9 4200 36.3294677734375\n",
            "9 4300 51.41691589355469\n",
            "9 4400 37.796600341796875\n",
            "9 4500 36.60267639160156\n",
            "9 4600 108.53483581542969\n",
            "9 4700 36.86644744873047\n",
            "9 4800 155.46066284179688\n",
            "10 0 86.01387023925781\n",
            "10 100 46.737300872802734\n",
            "10 200 71.30477905273438\n",
            "10 300 76.91493225097656\n",
            "10 400 35.64583206176758\n",
            "10 500 91.10963439941406\n",
            "10 600 27.14735221862793\n",
            "10 700 12.518563270568848\n",
            "10 800 37.86200714111328\n",
            "10 900 33.598655700683594\n",
            "10 1000 51.90475845336914\n",
            "10 1100 169.7784423828125\n",
            "10 1200 38.39519119262695\n",
            "10 1300 8.534907341003418\n",
            "10 1400 82.99668884277344\n",
            "10 1500 159.69940185546875\n",
            "10 1600 59.133216857910156\n",
            "10 1700 116.07166290283203\n",
            "10 1800 41.135520935058594\n",
            "10 1900 121.78495025634766\n",
            "10 2000 9.357731819152832\n",
            "10 2100 87.79691314697266\n",
            "10 2200 96.65764617919922\n",
            "10 2300 31.072284698486328\n",
            "10 2400 71.59126281738281\n",
            "10 2500 27.062721252441406\n",
            "10 2600 58.22742462158203\n",
            "10 2700 35.135108947753906\n",
            "10 2800 82.7635726928711\n",
            "10 2900 90.82604217529297\n",
            "10 3000 94.90505981445312\n",
            "10 3100 19.596351623535156\n",
            "10 3200 45.357208251953125\n",
            "10 3300 51.91117858886719\n",
            "10 3400 14.990788459777832\n",
            "10 3500 30.616291046142578\n",
            "10 3600 40.85477066040039\n",
            "10 3700 126.89712524414062\n",
            "10 3800 71.089111328125\n",
            "10 3900 16.56365394592285\n",
            "10 4000 38.790740966796875\n",
            "10 4100 257.1398010253906\n",
            "10 4200 35.37028503417969\n",
            "10 4300 41.94346618652344\n",
            "10 4400 39.36616897583008\n",
            "10 4500 26.58908462524414\n",
            "10 4600 88.55448150634766\n",
            "10 4700 38.75833511352539\n",
            "10 4800 157.0498046875\n",
            "11 0 108.58961486816406\n",
            "11 100 56.052677154541016\n",
            "11 200 68.01934051513672\n",
            "11 300 87.2869644165039\n",
            "11 400 28.198389053344727\n",
            "11 500 85.61625671386719\n",
            "11 600 23.39406394958496\n",
            "11 700 12.775601387023926\n",
            "11 800 37.46955871582031\n",
            "11 900 30.57335090637207\n",
            "11 1000 56.103973388671875\n",
            "11 1100 164.39447021484375\n",
            "11 1200 43.722782135009766\n",
            "11 1300 19.892986297607422\n",
            "11 1400 82.13619232177734\n",
            "11 1500 150.3308563232422\n",
            "11 1600 69.5737075805664\n",
            "11 1700 102.8126220703125\n",
            "11 1800 36.230613708496094\n",
            "11 1900 96.42526245117188\n",
            "11 2000 9.524091720581055\n",
            "11 2100 101.14788055419922\n",
            "11 2200 84.594970703125\n",
            "11 2300 32.14430618286133\n",
            "11 2400 72.21514129638672\n",
            "11 2500 33.01832580566406\n",
            "11 2600 53.49327850341797\n",
            "11 2700 25.529237747192383\n",
            "11 2800 98.89791870117188\n",
            "11 2900 110.08793640136719\n",
            "11 3000 97.10912322998047\n",
            "11 3100 20.346647262573242\n",
            "11 3200 35.21788024902344\n",
            "11 3300 57.64593505859375\n",
            "11 3400 17.414125442504883\n",
            "11 3500 30.064922332763672\n",
            "11 3600 41.24107360839844\n",
            "11 3700 123.97217559814453\n",
            "11 3800 63.2180061340332\n",
            "11 3900 14.830889701843262\n",
            "11 4000 42.73030090332031\n",
            "11 4100 246.42758178710938\n",
            "11 4200 30.459306716918945\n",
            "11 4300 44.229103088378906\n",
            "11 4400 39.921913146972656\n",
            "11 4500 27.34053611755371\n",
            "11 4600 98.27227020263672\n",
            "11 4700 33.33848571777344\n",
            "11 4800 143.22891235351562\n",
            "12 0 80.4764175415039\n",
            "12 100 50.97404098510742\n",
            "12 200 67.85310363769531\n",
            "12 300 84.08329010009766\n",
            "12 400 26.423185348510742\n",
            "12 500 89.87169647216797\n",
            "12 600 21.664546966552734\n",
            "12 700 8.62530517578125\n",
            "12 800 30.53973388671875\n",
            "12 900 30.57379150390625\n",
            "12 1000 55.42809295654297\n",
            "12 1100 162.72979736328125\n",
            "12 1200 34.58963394165039\n",
            "12 1300 11.69293212890625\n",
            "12 1400 101.18693542480469\n",
            "12 1500 151.04396057128906\n",
            "12 1600 52.593143463134766\n",
            "12 1700 103.0706787109375\n",
            "12 1800 41.57315444946289\n",
            "12 1900 115.72061920166016\n",
            "12 2000 9.77212905883789\n",
            "12 2100 92.39215850830078\n",
            "12 2200 97.53202056884766\n",
            "12 2300 38.095062255859375\n",
            "12 2400 78.32148742675781\n",
            "12 2500 34.86585235595703\n",
            "12 2600 57.601806640625\n",
            "12 2700 29.372976303100586\n",
            "12 2800 70.31781005859375\n",
            "12 2900 109.74176788330078\n",
            "12 3000 91.30172729492188\n",
            "12 3100 17.133527755737305\n",
            "12 3200 37.060482025146484\n",
            "12 3300 58.59027099609375\n",
            "12 3400 13.671675682067871\n",
            "12 3500 27.176206588745117\n",
            "12 3600 41.639862060546875\n",
            "12 3700 96.21985626220703\n"
          ]
        }
      ],
      "source": [
        "device = \"cpu\" # \"cuda\" nếu dùng GPU\n",
        "model = SimpleRNN(len(VOCAB), 150, len(VOCAB))\n",
        "train(model, trainset, 20, 0.005, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8elEcXwtTiFy"
      },
      "source": [
        "Cài đặt hàm tạo sinh câu từ mô hình RNN đã được huấn luyện."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PR_v0sGHWmo2"
      },
      "outputs": [],
      "source": [
        "def convert(words, dic):\n",
        "  samples = []\n",
        "  vocabSize = len(dic)\n",
        "  for stmp in words:\n",
        "    vtmp = np.zeros(vocabSize)\n",
        "    id = dic[stmp]\n",
        "    vtmp[id] = 1\n",
        "    samples.append(vtmp)\n",
        "\n",
        "  return samples\n",
        "def selectID(arr):\n",
        "  sel = random.random()\n",
        "  ex = np.exp(arr)\n",
        "  prob = ex/np.sum(ex)\n",
        "  b = 0\n",
        "  nwords = len(arr)\n",
        "  for i in range(nwords):\n",
        "    b += prob[i]\n",
        "    prob[i] = b\n",
        "  b = 0\n",
        "  id = 0\n",
        "  for i in range(nwords):\n",
        "    if sel > b and sel <= prob[i]:\n",
        "      id = i\n",
        "      break;\n",
        "    b = prob[i]\n",
        "  return id\n",
        "\n",
        "def generate(mdl: SimpleRNN, device, vocab, dictionary , s, n : int):\n",
        "  genText = []\n",
        "  sents = []\n",
        "  sents = s.split()\n",
        "  sample = convert(sents, vocab)\n",
        "\n",
        "  inputSize = len(sample[0])\n",
        "  h = mdl.initState()\n",
        "  x = torch.LongTensor([sample[0]])\n",
        "\n",
        "  out, h = mdl(x.to(device), h.to(device))\n",
        "  genText.append(sents[0])\n",
        "\n",
        "  for i in range(1, len(sents)):\n",
        "    x = [torch.LongTensor(sample[i])]\n",
        "    out, h = mdl(x.to(device), h.to(device))\n",
        "    genText.append(sents[i])\n",
        "\n",
        "  id = selectID(out.detach().cpu().numpy()[0])\n",
        "  predWord = dictionary[id]\n",
        "  genText.append(predWord)\n",
        "  if predWord == \"<END>\":\n",
        "    return genText\n",
        "  predArr = np.zeros(inputSize)\n",
        "  predArr[id] = 1\n",
        "  predX = torch.LongTensor([predArr])\n",
        "  for i in range(len(sents), n):\n",
        "    out, h = mdl(predX.to(device), h.to(device))\n",
        "    id = selectID(out.detach().cpu().numpy()[0])\n",
        "    predWord = dictionary[id]\n",
        "    genText.append(predWord)\n",
        "    predArr = np.zeros(inputSize)\n",
        "    predArr[id] = 1\n",
        "    predX = torch.LongTensor([predArr])\n",
        "    if predWord == \"<END>\":\n",
        "      return genText\n",
        "\n",
        "  return genText"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGbGcK_GTtLe"
      },
      "source": [
        "Tạo sinh 10 msg thuộc nhóm ham"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b56tMgaCZEZ8"
      },
      "outputs": [],
      "source": [
        "for i in range(10):\n",
        "  print(\" \".join(generate(model, device, VOCAB, DICT, \"<START>\", 20)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSxtxEesT3R7"
      },
      "source": [
        "BÀI THỰC HÀNH:\n",
        "\n",
        "Dựa vào các ví dụ, hãy hoàn thành nội dung thực hành của buổi hôm nay, gồm các điểm chính:\n",
        "\n",
        "- Huấn luyện mô hình ngôn ngữ cho ham và spam, dùng mô hình N-GRAM và RNN nào tốt nhất.\n",
        "\n",
        "- Tạo sinh 3000 sms cho tập ham và 3000 sms cho tập spam theo mỗi mô hình ngôn ngữ (N-GRAM và RNN)\n",
        "\n",
        "- Dùng mô hình phân lớp sms-spam tốt nhất đã huấn luyện được, để đánh giá trên hai bộ ngữ liệu đã tạo sinh ở bước trên."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}